{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Definition\n",
    "\n",
    "Logistic regression is a statistical method used for binary classification tasks. It models the probability of a binary outcome (1/0, True/False, Yes/No) based on one or more independent variables (features). The logistic regression model uses the logistic function (also known as the sigmoid function) to map a linear combination of features to a value between 0 and 1, which represents the probability of the binary outcome.\n",
    "\n",
    "The logistic function can be expressed as:\n",
    "\n",
    "\n",
    "`h(X)=g(θ.T*X)`\n",
    "\n",
    "`g(z)=1/(1+e^-z)`\n",
    "\n",
    "Where:\n",
    "- h(X) is the hypothesis function\n",
    "- θ is the parameter vector.\n",
    "- X is the feature vector.\n",
    "\n",
    "During training, the model learns the optimal values of  by minimizing a cost function, typically the log loss (cross-entropy), to make accurate predictions. Logistic regression is widely used in various applications, including spam detection, medical diagnosis, and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h(X) meaning\n",
    "\n",
    "`h(X) is the estimated probability that y=1 on input x`\n",
    "\n",
    "`h(X) = P(y=1 | X,θ)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Cost Function and Gradient Descent in Logistic Regression ---\n",
    "\n",
    "We can't use simple MSE(mean squared error) function as cost function for logistic regression as here it acts like a non-convex function\n",
    "\n",
    "- A `convex function` is one that assume to have a bowl like structure or 1 minima only\n",
    "\n",
    "Since,logistic regression only takes discreate value and does not span in a continuos range like regression problem and hence gradient Descent can't minimze it here\n",
    "\n",
    "# `--Cost-Function in Logistic regression--`\n",
    "Cost function in logistic regression is defined as : \n",
    "    \n",
    "    `J(θ)=(1/m)*∑ Cost(h(x),y)`\n",
    "\n",
    "    `Cost(h(x),y)= -y*ln(h(x)) - (1-y)*ln(1-h(x))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Gradient Descent --\n",
    "\n",
    "(repeat until conerge) {\n",
    "\n",
    "        θi := θi - alpha*(d/dθi*(J(θ)))\n",
    "\n",
    "    =>  θi := θi - alpha*(∑(h(x)-y).Xi)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(context='notebook', style='darkgrid',palette='dark', font_scale=1.2)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature1  Feature2  Target\n",
       "0       2.5       1.2       0\n",
       "1       3.9       2.8       0\n",
       "2       1.4       1.3       1\n",
       "3       4.6       3.5       0\n",
       "4       3.1       2.4       1"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"./logistic-reg-sampleDataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.5, 1.2],\n",
       "        [3.9, 2.8],\n",
       "        [1.4, 1.3],\n",
       "        [4.6, 3.5],\n",
       "        [3.1, 2.4]]),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]], dtype=int64))"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting features and target-varialbes from the dataset\n",
    "m=df.shape[0]\n",
    "X=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values.reshape((m,1))\n",
    "X[:5],y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 2), (10, 2), (40, 1), (10, 1))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spliting Data for Training and testing\n",
    "X_train, X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.1, num_iters=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "\n",
    "    def sigmoid(self,X):\n",
    "        sig = 1/(1+np.exp(-(X @ self.theta)))\n",
    "        return sig\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        def updateThetas(X,y):\n",
    "            y_hat = self.sigmoid(X)\n",
    "            error = y_hat-y\n",
    "\n",
    "            # Calculating Gradient\n",
    "            gradient = (1/m)*(X.T.dot(error))\n",
    "\n",
    "            # Updating thetas\n",
    "            self.theta -= self.learning_rate*gradient\n",
    "\n",
    "        m, n = X.shape\n",
    "        if y.shape[0] != m:\n",
    "            raise ValueError(\n",
    "                f\"Training and testing data should have same shape\\nX-shape{x.shape} & y-shape{y.shape}\")\n",
    "\n",
    "        # Initializing thetas\n",
    "        self.theta = np.random.rand(n+1).reshape((n+1, 1))\n",
    "\n",
    "        # Inserting vector with all 1's for intercept\n",
    "        X = np.append(np.ones((m, 1)), X, axis=1)\n",
    "\n",
    "        # storing no of features as model parameter as it would be helpfull\n",
    "        self.__no_of_features = (n+1)\n",
    "        for _ in range(self.num_iters):\n",
    "            updateThetas(X,y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        m, n = X.shape\n",
    "        # Checking if the dataset and training dataset are same or not as we have already appended this vector in training dataset and thus we have saved no of features also\n",
    "        if n != self.__no_of_features:\n",
    "            X = np.append(np.ones((m, 1)), X, axis=1)\n",
    "\n",
    "        sig=self.sigmoid(X)\n",
    "        y_pred = np.zeros((m, 1))\n",
    "        y_pred[sig >= 0.5] = 1\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 0., 0., 1., 0., 1., 0.]),\n",
       " array([[ 0.9329666 ],\n",
       "        [-0.5326162 ],\n",
       "        [ 0.20636083]]))"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My Models prediction\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred.ravel(),model.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sk-learns Model Prediction->\n",
    "sk_mod=LogReg()\n",
    "sk_mod.fit(X_train,y_train.ravel())\n",
    "sk_pred=sk_mod.predict(X_test)\n",
    "sk_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.93      0.88      0.89        10\n",
      "weighted avg       0.91      0.90      0.90        10\n",
      "\n",
      "\n",
      "sk-learns Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.70      0.71      0.70        10\n",
      "weighted avg       0.72      0.70      0.70        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification-reports for both models\n",
    "my_report=classification_report(y_test,y_pred)\n",
    "sk_report=classification_report(y_test,sk_pred)\n",
    "\n",
    "print(f\"My Report:\\n{my_report}\\n\")\n",
    "print(f\"sk-learns Report:\\n{sk_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Since this is not a really good-dataset to test logistic regression the values and acurracy fluctuate in every run. :(`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
